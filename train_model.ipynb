{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train house price predictor\n",
    "\n",
    "The objective of this notebook is to train a model that predicts housing prices. That model will be used in the Flask app to predict a price for an ad on Daft.ie. The user copy-pastes the ad URL into the browser, the app scrapes the ad from Daft.ie, transforms and enriches the data and returns a predicted price. \n",
    "\n",
    "The first part of this notebook explores the data and the second part trains the model. This notebook uses the MLpipeline class created in 'MLpipeline.py'. Some functions in that file will also be used live when predicting a price for a user of the app.\n",
    "\n",
    "The scraping and enriching has already been done and the data is stored in \"data/df_ads_mapdata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from MLpipeline import MLpipeline\n",
    "\n",
    "# This makes sure the that the plotly plots can be seen on github (static)\n",
    "# Remove this to get the interactive plots\n",
    "pio.renderers.default = \"svg\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "Loading the data and making a list of variables to be considered as features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads = pd.read_csv('data/df_ads_mapdata.csv')\n",
    "\n",
    "xlist = ['surface','area','property_type','ber_classification',\n",
    "         'selling_type','price_type','month','year','bathrooms','beds',\n",
    "         'dist_to_centre','caferestaurants', 'churches', 'health', \n",
    "         'parks', 'platforms', 'pubs','schools', 'shops', 'sports', \n",
    "         'stations', 'latitude', 'longitude','parking']\n",
    "all_vars = xlist\n",
    "all_vars.append('price')\n",
    "\n",
    "print(df_ads.info())\n",
    "\n",
    "print(\"Number of rows: {}\".format(df_ads.shape[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The info above shows the size of the dataset and shows that for some variables we have a considerable amount of missing values: \n",
    "\n",
    "- price type\n",
    "- BER classification (this is an energy rating)\n",
    "- surface\n",
    "\n",
    "Missing values will be imputed in the pipeline further on, but we should explore what is going on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning data\n",
    "When the app pulls an ad from the Daft.ie directly, this data needs to be cleaned as well. Therefore we need to define a function that does this cleaning. This function is defined in 'MLpipeline.py' and is called in this notebook later. Imputation of missing values is part of the preprocessing in the ML pipeline.\n",
    "\n",
    "Some of the cleaning, like removing observations, is only done for the purposes of training the model and does not need to be done everytime a new ad is pulled from the website. These cleaning actions will be done directly here in the notebook.\n",
    "\n",
    "The price type seems to only be non-missing if it is different from the norm, e.g. a region or a 'from' price. We can recode the missings to a 'normal' category. This is done in MLpipeline.py. The different prices types will have an influence on the price but because it is a significant number we keep the data. We include the variable in the model, however.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads.price_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the BER I cant discover anything unusual. Most likely the missing ratings are for properties of which the owner did not test the energy efficiency. The missing values will be imputed in the ML pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads.ber_classification.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The property type shows that most of the properties on the website are apartments, (semi-)detached, terraced or end-of-terrace properties. There are also some smaller categories: bungalows, duplexes, sites, studios and houses. Sites will be removed from the data for training the model. The aim is to predict prices for properties, not land. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ads.property_type.value_counts())\n",
    "\n",
    "df_ads = df_ads.drop(df_ads[df_ads.property_type=='site'].index)\n",
    "\n",
    "print(\"Number of rows: {}\".format(df_ads.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The selling type could considerably influence the price. Prices for houses that are auctioned will not be comparable to houses that are not auctioned. Only 'private-treaty' is kept and so are missing values which effectively imputes them as 'private-treaty'.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_ads.selling_type.value_counts())\n",
    "\n",
    "df_ads = df_ads[(df_ads!=\"auction\") & (df_ads!='private-tender') & (df_ads!='tender')]\n",
    "df_ads = df_ads.drop('selling_type',axis=1)\n",
    "\n",
    "print(\"Number of rows: {}\".format(df_ads.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for outliers in the numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads.loc[:,df_ads.columns.isin(all_vars)].describe(percentiles=[.001, .01, .05, .25, .5, .75, .95, .99, 0.999]).applymap(lambda x: format(x, \".1f\")).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be some extreme cases for the number of beds and bathrooms. Removing properties with more than 10 bedrooms or bathrooms. Also the surface area (square meters) can be rather high in a few cases which is either a mistake or it is properties with a signficant amount of land which is not surface area of the property itself. These will be removed because they would be very atypical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads = df_ads[df_ads.bathrooms<=10]\n",
    "df_ads = df_ads[df_ads.beds<=10]\n",
    "df_ads = df_ads[(df_ads.surface<=1000) | pd.isnull(df_ads.surface)]\n",
    "\n",
    "print(\"Number of rows: {}\".format(df_ads.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of the price is heavily skewed. House prices around 300k are most common but in our dataset it goes up to 10 million euros. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(x=df_ads.price, nbins=200, marginal=\"box\", labels={\"x\": \"price\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some properties are very cheap. The minimum price in the dataset is actually 20k. Less 0.1% of the data has a price of around 125k or lower. Anything under a 100k is probably an atypical property so they will be removed. Similarly, houses priced over 3 million euros will also be removed. Finally, dropping rows with missing prices. The plot below shows the price distribution after cleaning. \n",
    "\n",
    "For the estimation its probably better to take the log of the prices which returns a distribution that is much closer to the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads = df_ads[(df_ads.price>100000) & (df_ads.price<3000000)]       \n",
    "df_ads = df_ads.dropna(subset=['price'])\n",
    "\n",
    "print(\"Number of rows: {}\".format(df_ads.shape[0]))\n",
    "\n",
    "px.histogram(x=df_ads.price, nbins=200, marginal=\"box\", labels={\"x\": \"price\"}, log_x=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ads = df_ads.reset_index()\n",
    "print(\"Number of rows: {}\".format(df_ads.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlist = ['surface','area','property_type','ber_classification','price_type', 'month','year','bathrooms','beds','dist_to_centre','caferestaurants', 'churches', 'health',       'parks', 'platforms', 'pubs','schools', 'shops', 'sports', 'stations', 'latitude', 'longitude','facility','one_unit']\n",
    "\n",
    "numeric_features = ['surface','bathrooms', 'beds', 'dist_to_centre', 'caferestaurants', 'churches', 'health', 'parks', 'platforms', 'pubs','schools', 'shops', 'sports', 'stations', 'latitude', 'longitude']\n",
    "\n",
    "categorical_features = xlist.copy()\n",
    "\n",
    "for x in numeric_features:\n",
    "    categorical_features.remove(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "mlp_linreg = MLpipeline(df_ads)\n",
    "\n",
    "parametersGrid = {}\n",
    "\n",
    "mlp_linreg.set_estimator(LinearRegression(), \n",
    "                         parametersGrid, xlist,\n",
    "                         numeric_features, categorical_features)\n",
    "mlp_linreg.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_enet = MLpipeline(df_ads)\n",
    "\n",
    "parametersGrid = {\"estimator__alpha\": [0.001,0.01, 0.1,0.5,0.9,1],\n",
    "                  \"estimator__l1_ratio\": [0.01, 0.1, 0.3, 0.5, 0.7, 0.9]}\n",
    "\n",
    "mlp_enet.set_estimator(ElasticNet(max_iter=10000), \n",
    "                         parametersGrid, xlist, \n",
    "                         numeric_features, categorical_features)\n",
    "mlp_enet.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_rf = MLpipeline(df_ads)\n",
    "\n",
    "parametersGrid = {'estimator__n_estimators': [2500,3000,3500],\n",
    "                  'estimator__max_features': ['auto','log2'],\n",
    "                  'estimator__min_samples_leaf': [1,5,20,50]}\n",
    "\n",
    "mlp_rf.set_estimator(RandomForestRegressor(random_state=42), \n",
    "                     parametersGrid, xlist,\n",
    "                     numeric_features, categorical_features) \n",
    "\n",
    "mlp_rf.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_xgb = MLpipeline(df_ads)\n",
    "\n",
    "parametersGrid = {'estimator__n_estimators': [2500,3000,3500],\n",
    "                  'estimator__max_depth': [1,3,5,7,9,11],\n",
    "                  'estimator__learning_rate': [0.01,0.1,0.3],\n",
    "                  'estimator__gamma': [0.0, 0.2, 0.4],\n",
    "                  'estimator__colsample_bytree': [0.1,0.3,0.5]}\n",
    "\n",
    "mlp_xgb.set_estimator(XGBRegressor(objective='reg:squarederror', random_state=42),\n",
    "                         parametersGrid, xlist,\n",
    "                         numeric_features, categorical_features)\n",
    "mlp_xgb.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.DataFrame({'y_pred': mlp_linreg.y_pred,\n",
    "                    'y_test': mlp_linreg.y_test,\n",
    "                    'estimator': 'Regression'})\n",
    "\n",
    "df1 = pd.DataFrame({'y_pred': mlp_enet.y_pred,\n",
    "                    'y_test': mlp_enet.y_test,\n",
    "                    'estimator': 'ElasticNet'})\n",
    "\n",
    "df2 = pd.DataFrame({'y_pred': mlp_rf.y_pred,\n",
    "                    'y_test': mlp_rf.y_test,\n",
    "                    'estimator': 'RandomForest'})\n",
    "              \n",
    "df3 = pd.DataFrame({'y_pred': mlp_xgb.y_pred,\n",
    "                    'y_test': mlp_xgb.y_test,\n",
    "                    'estimator': 'GradientBoost'})\n",
    "\n",
    "df = df0.append(df1, ignore_index=True)\n",
    "df = df.append(df2, ignore_index=True)\n",
    "df = df.append(df3, ignore_index=True)\n",
    "\n",
    "fig = px.scatter(df, x=\"y_test\", y=\"y_pred\", color=\"estimator\", \n",
    "                marginal_y=\"box\",\n",
    "                labels={'y_test':'Observed',\n",
    "                        'y_pred': 'Predicted'})\n",
    "                        \n",
    "fig.update_traces(marker=dict(size=6,\n",
    "                              opacity=0.5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model fit seems to be comparable for the different estimators. Generally it seems that overfitting is not an issue, e.g. linear regression works better than elastic net and the best paramters of the gridsearch for elastic net are those very close to linear regression. Gradient stochastic boosting outperforms the others slightly so this model will be used to predict housing prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_xgb.save_model('model/model.pkl')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.3-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37364bitbaseconda627861933211447fa3d0dae2dcc65a67",
   "display_name": "Python 3.7.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}